---
title: "Analysis on IMDB Movie Reviews"
author: "Sumanth"
date: "2/28/2020"
output:
  html_document:
    df_print: paged
---

### Introduction 

In this Assignment, we are going to explore the concepts of Layer Embedding, Pre-trained word embedding, tokenization and various techniques to observe the variations in the Accuracy. We will use movie reviews dataset which is pre-tokenized IMDB data packaged in Keras for layer embeddings. We will also download original raw text data for pre-trained word embeddings and compare our findings with different training and validation samples on the data.
IMDB text dataset One of the best ways to understand the concepts of tokenization etc., with RNN. It provides a reasonable baseline to assess further complex models. The objective of this assignment is to change the training samples, applying different layer embeddings to observe changes in the accuracy train data set and to determine the performance.
  
```{r setup, include=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```

**Building a model with sample word embedding layer**
```{r}
maxlen <- 150
training_samples <- 100
validation_samples <- 10000
max_words <- 10000
#Keras-IMDB dataset
data  <- dataset_imdb(num_words = max_words)
c(c(train_data,train_labels),c(test_data,test_labels)) %<-% data

#Padding the text data
x_train <- pad_sequences(train_data,maxlen=150)
x_test <- pad_sequences(test_data,maxlen = 150)
#Partition of Traning data into validation & traning
set.seed(1234)
partial_x_train <- x_train[1:training_samples,]
x_val <- x_train[200:10200,]
partial_y_train <- train_labels[1:training_samples]
y_val <- train_labels[200:10200]

#Model Building
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim =8,input_length = maxlen) %>%
  layer_flatten() %>% 
  layer_dense(units=1,activation = "sigmoid")

model %>% compile(optimizer="rmsprop",
                  loss ="binary_crossentropy",
                  metrics=c("acc")
                  )
#Model Training
history <- model %>% fit(partial_x_train,partial_y_train,epochs =10,batch_size=32,validation_data = list(x_val,y_val))
plot(history)
#Evaluate the model on test data 
results <- model %>% evaluate(x_test,test_labels)
results
# By observing the plot, the validation accuracy of the model is ~70% considering the first 150 words in every review with 100 samples.
# Test Acuuracy of the model is 51%
```

**Building a model with pre-trained network from scratch**
```{r}
library(keras)
maxlen <- 150
training_samples <- 100
validation_samples <- 10000
maxwords <- 10000

imdb_dir <- "C:/Users/suman/Downloads/aclImdb"
train_dir <- file.path(imdb_dir, "train")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}

tokenizer <- text_tokenizer(num_words = maxwords) %>% fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

# Split the data into a training set and a validation set
# But first, shuffle the data, since we started from data
# where sample are ordered (all negative first, then all positive).
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                                (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]


#GloVe
glove_dir = "C:/Users/suman/Documents/glove.6B"
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")
#embedding matrix
embedding_dim <- 100

embedding_matrix <- array(0, c(maxwords, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < maxwords) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
#build the model
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = maxwords, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history1 <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")
plot(history1)

```
Observing the above plot, we can say the validaiton accuracy of the model is arounnt 50% with a sample size of 100 in the training dataset.The model quickly starts overfits with a small number of traning samples. 
Hence, performance is highly dependent on random samples of 100.

* Naturally, model performs better with more training data for pretrained network.
